\documentclass[journal,12pt,twocolumn]{IEEEtran}
%
\usepackage{setspace}
\usepackage{gensymb}
\usepackage{xcolor}
\usepackage{caption}
%\usepackage{subcaption}
%\doublespacing
\singlespacing

%\usepackage{graphicx}
%\usepackage{amssymb}
%\usepackage{relsize}
\usepackage[cmex10]{amsmath}
\usepackage{mathtools}
%\usepackage{amsthm}
%\interdisplaylinepenalty=2500
%\savesymbol{iint}
%\usepackage{txfonts}
%\restoresymbol{TXF}{iint}
%\usepackage{wasysym}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}
%\usepackage{xtab}
\usepackage{longtable}
\usepackage{multirow}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{mathtools}
%\usepackage{iithtlc}
%\usepackage[framemethod=tikz]{mdframed}
\usepackage{listings}


%\usepackage{stmaryrd}


%\usepackage{wasysym}
%\newcounter{MYtempeqncnt}
\DeclareMathOperator*{\Res}{Res}
%\renewcommand{\baselinestretch}{2}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\lstset{
language=Python,
frame=single, 
breaklines=true
}

%\lstset{
	%%basicstyle=\small\ttfamily\bfseries,
	%%numberstyle=\small\ttfamily,
	%language=python,
	%backgroundcolor=\color{white},
	%%frame=single,
	%%keywordstyle=\bfseries,
	%%breaklines=true,
	%%showstringspaces=false,
	%%xleftmargin=-10mm,
	%%aboveskip=-1mm,
	%%belowskip=0mm
%}

%\surroundwithmdframed[width=\columnwidth]{lstlisting
\begin{document}
%

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}{Definition}[section]
%\newtheorem{definition}{Definition}
%\newtheorem{algorithm}{Algorithm}[section]
%\newtheorem{cor}{Corollary}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}

\bibliographystyle{IEEEtran}
%\bibliographystyle{ieeetr}

\providecommand{\nCr}[2]{\,^{#1}C_{#2}} % nCr
\providecommand{\nPr}[2]{\,^{#1}P_{#2}} % nPr
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\lVert#1\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
%\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\numberwithin{equation}{section}
%\numberwithin{equation}{problem}
%\numberwithin{problem}{subsection}
%\numberwithin{definition}{subsection}
\providecommand{\mean}[1]{E\left[ #1 \right]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
%\providecommand{\hilbert}{\overset{\mathcal{H}}{ \rightleftharpoons}}
\providecommand{\system}{\overset{\mathcal{H}}{ \longleftrightarrow}}
	%\newcommand{\solution}[2]{\textbf{Solution:}{#1}}
%\newcommand{\solution}{\noindent \textbf{Solution: }}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\numberwithin{equation}{section}
%\numberwithin{equation}{problem}
%\numberwithin{problem}{subsection}
%\numberwithin{definition}{subsection}
\makeatletter
\@addtoreset{figure}{problem}
\makeatother

\let\StandardTheFigure\thefigure
\let\vec\mathbf
%\renewcommand{\thefigure}{\theproblem.\arabic{figure}}
\renewcommand{\thefigure}{\theproblem}


%\numberwithin{figure}{subsection}

\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}

\vspace{3cm}

\title{ 
%\logo{
Concentration Inequalities
%}
%	\logo{python for Math Computing }
}
%\title{
%	\logo{Matrix Analysis through python}{\begin{center}\includegraphics[scale=.24]{tlc}\end{center}}{}{HAMDSP}
%}


% paper title
% can use linebreaks \\ within to get better formatting as desired
%\title{Matrix Analysis through python}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Sumohana Chennappayya and G V V Sharma$^{*}$ %<-this  stops a space
\thanks{ *The author is with the Department
of Electrical Engineering, IIT, Hyderabad
502285 India e-mail: gadepall@iith.ac.in. All material in the manuscript is 
released under GNU GPL.  Free to use for all.}% <-this % stops a space
%\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised January 11, 2007.}}
}
% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~6, No.~1, January~2007}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2007 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% make the title area
\maketitle

%\documentclass{article}
%\usepackage{amsmath}
%\begin{document}
%\centerline{\textbf{EE5603:Conceention Inequalities}}
%\section{Convergence}
%\subsection{Definitions}
%%\begin{itemize}
%%\item Today :\item Recall motivation of measure theroetic prob.
%%\item Basic inequalities with proof and examples
%%\ - markor 
%%\ - Chebyshow
%%\ - Chernoff
%%\ - LLN
%%\end{itemize}
%\section{Basice into to measure theroetic probability:}
%$\bullet(\Omega, \int,\rho )$ the probility triplet\\
%
%$\Omega :$ set of proible ontcones w.\\
%
%$\int$ : {$\sigma$}-algebra defined on $\Omega$ that satsti the following axiome\\
%
%\section{Axionss}
%A.1. $\Omega \epsilon$\\
%
%A.2.if A $\epsilon \int ,A^c \epsilon $\\
%
%A.3.if A $\epsilon ,B \epsilon , then A UB \epsilon $\\
%
%From A.2,(OR) A.3 we can show that if A $\varepsilon$ $\int$,B $\epsilon$ $\int$\\
%
%then,A$\bigcap$B $\epsilon$ $\int$\\
%
%$\textbf{proof}:A^c \epsilon \int, B^c \epsilon \int$ (from A.2)\\
%
%$\Rightarrow A^c \cup B^c \varepsilon \int$(from A.3)\\
%
%$\Rightarrow(A^c \cup B^c)^c \epsilon \int$ (from A.2)\\
%
%we know $ A \cup B =(A^c \cup B^c)^c$\\
%
%$\therefore A \cap B \epsilon \int.$\\
%
%\textbf{P}:A probalitity measure defined on that satises the following axiome\\
%
%\textbf{P.1:}$\Pr(A)$ $\geqslant$ 0 for all A$\varepsilon$\\
%
%\textbf{P.2:}$ \Pr(A_1 \cup A_2)$ = $\Pr(A_1)$+ $Pr(A_2)$ for distint sets $(A_1,A_2)$\\
%
%\textbf{P.3:}$\Pr(\Omega)=1$\\
%
%A random variable x map $\Omega$ to  and is $\int$ -measurable.\\
%
%for any $\varepsilon $, {w:X(W)$\leq $ $\varepsilon$} $\varepsilon$ $\epsilon$\\
%
%\section{Recall defin of a.s. conesgence:}
%\begin{align}
%\Pr(line|x_n=x)=1
%\end{align}
%can be interpreted as\\
%\begin{align}
%\Pr(w: x_n (w)=x(w))=1
%\end{align}
%\textbf{ex:} $\Omega$={a,b,c,d}\\
%$\sigma$-algebra ={$\Omega$, $\emptyset$,{a,b},{c,d}}\\
%\begin{align}
%\bullet F_x(x)=\Pr{w:x(w) \leqslant x}
%\end{align}
%\begin{align}
% =\Pr(X \leq \lambda)
% \end{align}
% \begin{align}
%\bullet F_x(\lambda)=\int_{-\propto}^{x}f_x(t)dt
%\end{align} 
%$\bullet$ Review /prowe basic ineqlities:
\section{Markov Inequality}
\begin{enumerate}[label=\thesection.\arabic*,ref=\thesection.\theenumi]
\item Let $X \ge 0$ be a positive random integer. Show that
\begin{align}
E[X]=\sum_{m=0}^{\infty} \Pr\brak{X\geqslant m}
\end{align}
\solution By definition,
\begin{align}
\label{eq:mean_prob}
E[X]&=\sum_{m=0}^{\infty} m\Pr\brak{X = m}
\\
&= \Pr\brak{X = 1}+2\Pr\brak{X = 2}+3\Pr\brak{X = 3}
\nonumber\\
&\,\,+\dots
\\
&= \lcbrak{\Pr\brak{X = 1}+\Pr\brak{X = 2}+\Pr\brak{X = 3}}
\nonumber \\
&\,\,+\rcbrak{\dots}
\\
&+ \cbrak{\Pr\brak{X = 2}+\Pr\brak{X = 3}+\dots}
\\
&+ \cbrak{\Pr\brak{X = 3}+\dots} + \dots
\\
&= \Pr\brak{X \geqslant 1}+2\Pr\brak{X \geqslant 2}+3\Pr\brak{X \geqslant 3}
\nonumber\\
&\,\,+\dots
\end{align}
resulting in \eqref{eq:mean_prob}.
\item For a continuious r.v $X \ge 0$, show that 
\begin{align}
E\sbrak{X}=\int_{0}^{\infty} \Pr(x\geqslant t)dt
\end{align}

\item For r.v $X \ge 0$ and $\varepsilon> 0$, show that 
\begin{align}
\label{eq:markov}
\Pr\brak{X \geqslant \varepsilon} \leqslant\frac{E\sbrak{X}}{\varepsilon}
 \end{align}
\solution $\because X \geqslant 0$, 
\begin{align}
E\sbrak{X}&=\int_{0}^{\infty} x p_X(x)\,dx
%=\int_{0}^{\infty} x.f_x(\lambda)dx(\therefore x\geqslant 0)
\\
&=\int_{0}^{\varepsilon} x p_X(x)\,dx +\int_{\varepsilon}^{\infty}x p_X(x)\,dx
\\
&\geqslant \int_{\varepsilon}^{\infty}x p_X(x)\,dx 
\end{align}
which can be expressed as
\begin{align}
E\sbrak{X} &\geqslant \int_{\varepsilon}^{\infty}\varepsilon p_X(x)\,dx
\\
& = \varepsilon \int_{\varepsilon}^{\infty} p_X(x)\,dx
 = \varepsilon \pr{X \geqslant \varepsilon}
\end{align}
resulting in $\eqref{eq:markov}$.
\item {\em Chernoff Bound :} For any r.v $X$ with bounded variance, and for any $t >0$, show using \eqref{eq:markov} that %
\begin{align}
\label{eq:chernoff}
\Pr(e^{tX}\geqslant e^{t\varepsilon})\leqslant \frac{E\brak{e^{tX}}}{e^{t\varepsilon}}
\end{align}
\item Show that 
\begin{align}
\label{eq:markov_chernoff}
\Pr(X\geqslant \varepsilon) = \Pr(e^{tX}\geqslant e^{t\varepsilon})
\end{align}
\solution This is true for any monotonic function.  
\end{enumerate}
                                                                                                                                                                                                                                            \section{Chebyschev inequality} 
\begin{enumerate}[label=\thesection.\arabic*,ref=\thesection.\theenumi]
\item Let
\begin{align}
\label{eq:chebyschev_Y} 
Y = \brak{X-E[X]}^2
 \end{align}
and $\varepsilon >0$.
Show using \eqref{eq:markov} that
\begin{align} 
 \Pr\brak{Y\geqslant\varepsilon^2}\leqslant \frac{E\brak{Y}}{\varepsilon^2}
 \end{align}
\item Show that
\begin{align} 
 \Pr\brak{Y\geqslant\varepsilon^2}=\Pr\brak{\sqrt{Y}\geqslant\varepsilon}+\Pr\brak{\sqrt{Y}\leqslant-\varepsilon}
\label{eq:chebyschev_posneg}
 \end{align}
%
\item Show that
\begin{align}
\Pr\brak{\sqrt{Y}\leqslant-\varepsilon} = 0,
 \end{align}
\item Show that 
\begin{align}
\Pr\brak{\sqrt{Y}\geqslant\varepsilon}\leqslant \frac{E\brak{Y}}{\varepsilon^2}
 \end{align}
\item Show that
\begin{align}
\label{eq:chebyschev}
 \Pr\brak{\abs{X-E[X]}\geqslant\varepsilon}\leqslant \frac{\text{Var}(X)}{\varepsilon^2}
 \end{align}
\end{enumerate}
\section{Law of large numbers (LLN)}
\begin{enumerate}[label=\thesection.\arabic*,ref=\thesection.\theenumi]
\item Let
\begin{align}
S_n=\frac{1}{n}\sum_{i=1}^{n} X_i 
\end{align}
where $X_i$ are i.i.d r.v. with mean $\mu$  and bounded variance $\sigma^2$.
Show that 
\begin{align}
E\brak{S_n}&=\mu
\\
\text{Var}\brak{S_n}&=\frac{\sigma^2}{n}
\end{align}
\item Using Chebyschev inequality in \eqref{eq:chebyschev},  show that 
\begin{align}
\Pr\brak{\abs{S_n-\mu}\geqslant \varepsilon}\leqslant \frac{\sigma^2}{n\varepsilon^2}
\end{align}
\item Show that 
\begin{align}
\lim_{n\to \infty}\Pr\brak{\abs{S_n-\mu}\geqslant \varepsilon} = 0
\end{align}
\end{enumerate}
\section{Hoeffding's Lemma}
\begin{enumerate}[label=\thesection.\arabic*,ref=\thesection.\theenumi]
\item Consider a r.v. $X$ such that  $a < X < b$. If $\theta = \frac{a}{a-b}$ and $E\sbrak{X} = 0$, show that 
\begin{align}
0 < \theta < 1
\end{align}
\item A convex function $g$ is defined as
\begin{align}
g\brak{x} \leqslant f(x), \quad x \in \brak{a,b}
\end{align}
where $f$ is the line joining the points $\brak{a,g(a)}$ and $\brak{b,g(b)}$.  Show that
\begin{align}
\label{eq:hoeff_convex}
E\sbrak{f(X)} \le \frac{bg(a)- ag(b)}{b-a}
\end{align}
%
\item Show that 
\begin{align}
g\brak{x} = e^{sx}, s > 0
\label{eq:exp_convex}
\end{align}
is convex.
\item Using \eqref{eq:hoeff_convex} and \eqref{eq:exp_convex}, show that the moment generating function (MGF) of $X$
\begin{align}
\label{eq:hoeff_mgf}
M_{X}(s) = E\sbrak{e^{sX}}  \le e^{-u\theta}\brak{1-\theta + \theta e^u},
\end{align}
%
where $u = s(b-a)$.
\item Let 
\begin{align}
\label{eq:hoeff_psi}
e^{-u\theta}\brak{1-\theta + \theta e^u} = e^{\psi(u)}
\end{align}
Show that 
\begin{align}
\label{eq:hoeff_psi_der}
\psi(0) &= 0
\\
\psi^{\prime}(0) &= 0
\\
\psi^{\prime\prime}(v) &= t \brak{1-t}, \quad t = \frac{\theta e^{v}}{1 - \theta + \theta e^{v}}.
\end{align}
\item Show that 
\begin{align}
\label{eq:hoeff_psi_der2}
\psi^{\prime\prime}(v) \le \frac{1}{4}
\end{align}
\item According to Taylor's theorem,
\begin{align}
\label{eq:hoeff_taylor}
\psi(u) = \psi(0)  + u\psi^{\prime}(0) + \frac{u^2}{2}\psi^{\prime\prime}(v) \quad 0 < v < u.
\end{align}
\item Using \eqref{eq:hoeff_psi_der}, \eqref{eq:hoeff_psi_der2} and \eqref{eq:hoeff_taylor}, show that 
\begin{align}
\label{eq:hoeff_psi_ineq}
\psi(u) \le  \frac{u^2}{8} \quad 0 < v < u.
\end{align}
\item From \eqref{eq:hoeff_mgf}, \eqref{eq:hoeff_psi} and \eqref{eq:hoeff_psi_ineq}, show that 
\begin{align}
\label{eq:hoeff_lemma}
M_{X}(s) \le  e^\frac{s^2\brak{b-a}^2}{8} 
\end{align}

%\item ({\em Jensen's Inequality}) Show that
%\begin{align}
%g\brak{E\sbrak{X}} \leqslant E\sbrak{g(X)}
%\end{align}
\end{enumerate}
\section{Hoeffding's Inequality}
\begin{enumerate}[label=\thesection.\arabic*,ref=\thesection.\theenumi]
\item Let 
\begin{align}
S_n=\sum_{i=1}^{n} X_i, \quad X_i \in \sbrak{a_i,b_i}, E\sbrak{X_i}  = 0
\end{align}
%
wherer $X_i$ are independent and 
\begin{align}
Y = S_n- E\sbrak{S_n}.
\end{align}
%
\item Using \eqref{eq:chernoff} and \eqref{eq:markov_chernoff}, show that 
\begin{align}
\label{eq:hoeff_chernoff}
\pr{Y \ge t} \le e^{-st} M_{Y}(s).
\end{align}
\item Show that 
\begin{align}
\label{eq:hoeff_mgf_prod}
M_{Y}(s)= \prod_{i=1}^{n}M_{X_i}(s)
\end{align}
\item From \eqref{eq:hoeff_chernoff}, \eqref{eq:hoeff_mgf_prod} and 
\eqref{eq:hoeff_lemma}, show that 
\begin{align}
%\label{eq:hoeff_mgf_prod}
\pr{Y \ge t} \le \exp\brak{-st+\frac{1}{8}s^2\norm{\vec{b}-\vec{a}}^2}
\end{align}
\item Show that 
\begin{align}
\label{eq:hoeff_min_ineq}
\min_{s}\brak{-st+\frac{1}{8}s^2\norm{\vec{b}-\vec{a}}^2} = -\frac{2t^2}{\norm{\vec{b}-\vec{a}}^2}
\end{align}
\item Show that 
\begin{align}
\label{eq:hoeff_ineq}
\pr{S_n- E\sbrak{S_n} \ge t} \le \exp\brak{-\frac{2t^2}{\norm{\vec{b}-\vec{a}}^2}}
\end{align}
\end{enumerate}

\section{Bennet's Inequality}
\begin{enumerate}[label=\thesection.\arabic*,ref=\thesection.\theenumi]
\item A real valued r.v $X$ is  said to be $\sigma^2$-sub Gaussian if there exists a $\sigma$ such that 
\begin{align}
M_{X}(\lambda) < e^{\frac{\lambda^2 \sigma^2}{2}}
\end{align}
\item Let
\begin{align}
\phi(x) = e^x - x -1.
\end{align}
%
Show that $u^{-2}\phi(u)$ is non-decreasing.
\item  Show that
\begin{align}
\brak{\lambda X_i}^{-2} \phi \brak{\lambda X_i}\leqslant \lambda^{-2} \phi (\lambda), \quad \lambda > 0, X_i < 1.
\end{align}
\item Show that
\begin{align}
E[e^{\lambda X_i}]\leqslant 1+\lambda E\sbrak{ X_i}+E\sbrak{X_i^2}\phi (\lambda)
\end{align}
\item Let 
\begin{align}
S = \sum_{1=1}^{n} X_i - E\sbrak{X_i}.
\end{align}
%
Show that 
\begin{multline}
\label{eq:bennet_mgf}
\log M_{S}(\lambda) \leqslant  
\\
\sum_{i=1}^{n} \log\sbrak{1+\lambda E\brak{ X_i}+E\brak{X_i^2} \phi (\lambda)} 
\\
- \lambda \sum_{i=1}^{n}E \brak{X_i}
\end{multline}
\item Given that $\log$ is convex, show that
\begin{multline}
\sum_{1=1}^{n} \log \sbrak{1+\lambda E\brak{X_i}+E\brak{X_i^2}\phi (\lambda)}
\\
\le 
n \log \sbrak{\frac{1}{n} \sum_{1=1}^{n}\brak{1+\lambda E\brak{X_i}+E\brak{X_i^2}\phi (\lambda)}}
\end{multline}
\item Using the fact that $\log (1+x) \leqslant x, x \geqslant 0$, show that 
\begin{multline}
\label{eq:bennet_log_ineq}
n \log \sbrak{\frac{1}{n} \sum_{1=1}^{n}\brak{1+\lambda E\brak{X_i}+E\brak{X_i^2}\phi (\lambda)}}
\\
- \lambda \sum_{i=1}^{n}E \brak{X_i}
\le
v\phi (\lambda)
\end{multline}
%
where
\begin{align}
v &= \sum_{1=1}^{n} E \brak{X_i^2},
\\
\sum_{1=1}^{n} E \brak{X_i} & \ge 0,
\end{align}
\item From \eqref{eq:bennet_mgf}and \eqref{eq:bennet_log_ineq}, show that
\begin{align}
\log M_{S}(\lambda) \le \frac{v}{b^2}\phi(\lambda b),
\end{align}
for $X_i \le b$.
\end{enumerate}

\section{Mc Diarmid's inequality}
\begin{enumerate}[label=\thesection.\arabic*,ref=\thesection.\theenumi]
\item Let 
\begin{align}
\vec{X}_i &= \vec{X}_{1:i}=\myvec{X_1 & X_2 & \dots & X_i},\, \text{(say)}
\\
\vec{X} &\define \vec{X}_n 
\\
B_i &\define E\sbrak{g\brak{\vec{X}} |\vec{X}_i}
\end{align}
where $X_i$ are all independent.
Show that
\begin{align}
B_n &= g\brak{\vec{X}}
\\
B_0 &= E\sbrak{g\brak{\vec{X}}}
\\
B_i &= \sum_{\vec{a}_{i+1:n}}g\brak{\vec{X}_{i},\vec{a}_{i+1:n}}\pr{\vec{X}_{i+1:n} = \vec{a}_{i+1:n}}
\end{align}
%\item Show that 
%\begin{align}
%\abs{x-y} \le c \implies \abs{x-py} \le c, \quad 0 < p < 1
%\end{align}

%Where $X_k, k = 1, 2, \dots, i$  are independent. If 
\item If
\begin{multline}
|g(x_{1},x_{2},\dots ,x_{n})
\\
-g(x_{1},x_{2},\dots ,x_{{i-1}},{\hat  x}_{i},x_{{i+1}},\dots ,x_{n})|\leq c_{i}
\\
 {\text{for}}\quad 1\leq i\leq n,
\end{multline}
%
show that
\begin{multline}
|g\brak{\vec{X}_{i},\vec{a}_{i+1:n}}
\\
-\sum_{{a}_{i}^{\prime}}g\brak{\vec{X}_{i},a_i^{\prime},\vec{a}_{i+1:n}}\pr{{X}_{i} = {a}_{i}^{\prime}}
|
\\
\le c_i
\label{eq:diarmid_ineq}
\end{multline}
\solution
\begin{align}
\because 
\sum_{{a}_{i}^{\prime}}\pr{{X}_{i} = {a}_{i}^{\prime}} = 1,
\end{align}
\eqref{eq:diarmid_diff} can be expressed as
\begin{multline}
\sum_{{a}_{i}^{\prime}}|g\brak{\vec{X}_{i},\vec{a}_{i+1:n}}
-g\brak{\vec{X}_{i},a_i^{\prime},\vec{a}_{i+1:n}}|
\\
\times \pr{{X}_{i} 
= {a}_{i}^{\prime}}
\le 
 c_i
\end{multline}
\item If
\begin{align}
V_i &=B_i - B_{i-1}, 
\end{align}
show that 
\begin{align}
\abs{V_i} \le c_i
\end{align}
\solution
\begin{multline}
\because V_i =B_i - B_{i-1} 
\\
= 
\sum_{\vec{a}_{i+1:n}}g\brak{\vec{X}_{i},\vec{a}_{i+1:n}}\pr{\vec{X}_{i+1:n} = \vec{a}_{i+1:n}}
\\
- 
\sum_{\vec{a}_{i:n}}g\brak{\vec{X}_{i-1},\vec{a}_{i:n}}\pr{\vec{X}_{i:n} = \vec{a}_{i:n}}
\end{multline}
%
we obtain
\begin{multline}
\abs{ V_i}
\le 
\sum_{\vec{a}_{i+1:n}}|g\brak{\vec{X}_{i},\vec{a}_{i+1:n}}
\\
-\sum_{{a}_{i}^{\prime}}g\brak{\vec{X}_{i},a_i^{\prime},\vec{a}_{i+1:n}}\pr{{X}_{i} = {a}_{i}^{\prime}}
|
\\
\pr{\vec{X}_{i+1:n} = \vec{a}_{i+1:n}}
\label{eq:diarmid_diff}
\end{multline}
Using \eqref{eq:diarmid_ineq} in \eqref{eq:diarmid_diff},

\begin{align}
\abs{V_i} \le c_i \sum_{\vec{a}_{i+1:n}}\pr{\vec{X}_{i+1:n} = \vec{a}_{i+1:n}} = c_i
%\because 
%\sum_{{a}_{i}^{\prime}}\pr{{X}_{i} = {a}_{i}} = 1,
\end{align}
%\eqref{eq:diarmid_diff} can be expressed as
%\begin{multline}
%\sum_{{a}_{i}^{\prime}}|g\brak{\vec{X}_{i},\vec{a}_{i+1:n}}
%-g\brak{\vec{X}_{i},a_i^{\prime},\vec{a}_{i+1:n}}|
%\\
%\times \pr{{X}_{i} 
%= {a}_{i}^{\prime}}
%\le 
% c_i
%\end{multline}
\item Show that 
\begin{align}
 E\sbrak{V_i}  = 0
\end{align}
\item Let
\begin{align}
S_n=\sum_{i=1}^{n} V_i, \quad V_i \in \sbrak{-c_i,c_i}, E\sbrak{V_i}  = 0
\end{align}
%
Show that
\begin{align}
S_n = B_n - B_0 = g\brak{\vec{X}}-E\sbrak{g\brak{\vec{X}}}
\end{align}
\item If 
\begin{align}
\vec{c}_i = \myvec{c_1 & c_2 & \dots & c_n},
\end{align}
substituting $\vec{a} = -\vec{c}, \vec{b} = \vec{c}$ in \eqref{eq:hoeff_ineq}, show that
\begin{align}
\label{eq:mcdiarmid}
\pr{S_n- E\sbrak{S_n} \ge t} &\le \exp\brak{-\frac{t^2}{2\norm{\vec{c}}^2}}
\\
\pr{S_n- E\sbrak{S_n} \le -t} &\le \exp\brak{-\frac{t^2}{2\norm{\vec{c}}^2}}
\end{align}

\end{enumerate}

\section{Efrom-Stien Inequality}
\begin{enumerate}[label=\thesection.\arabic*,ref=\thesection.\theenumi]
\item Let
\begin{align}
\vec{V}= \sum_{i=1}^{n}{V}_i = Z - E\brak{Z}
\end{align}
Show that 
\begin{align}
\text{var} \brak{Z} = \sum_{1=1}^{n}E\brak{ V_i^2}+2\sum_{i>j}E\brak{V_i V_j}
\end{align}
\item If
\begin{align}
\vec{X}_i = \vec{X}_{1:i}=\myvec{X_1 & X_2 & \dots & X_i},\, \text{(say)}
\end{align}
%
and 
\begin{align}
{V}_i =E_{i+1:n}\brak{Z|\vec{X}_{i}}-E_{i:n}\brak{Z|\vec{X}_{i-1}} 
\end{align}
show that 
\begin{align}
E\brak{V_i V_j} = E\sbrak{V_i E_{i+1:n}\brak{V_j|\vec{X}_i}} = 0, \quad i > j
\end{align}
and
\begin{align}
\text{var} \brak{Z} = \sum_{i=1}^{n}E\brak{ V_i^2} 
\end{align}
\item Show that 
\begin{align}
E_{i:n}\brak{Z|\vec{X}_{i-1}} = E_{i+1:n}\sbrak{E_{i}\brak{Z|\vec{X}_{i-1}} }
\end{align}
\item Using Jensen's inequality, show that 
\begin{align}
E\brak{ V_i^2} \le E\sbrak{\cbrak{Z-E_i\brak{Z|X_{1:i,i+1,n}}}^2}
%E\sbrak{E_i(Z)-E_{i-1}(Z) }
\end{align}
\end{enumerate}
\end{document}
